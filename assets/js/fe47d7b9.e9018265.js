"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[480],{5168(n,e,r){r.d(e,{A:()=>h});r(6540);var i=r(8774);const t="container_dCqE",o="card_xNje",s="icon_xPcq",a="content_EpEg",l="label_oX12",d="title_FgAO",c="duration_fnZY",m="button_ldru";var p=r(4848);function h({episodeUrl:n,episodeNumber:e,duration:r}){return(0,p.jsx)("div",{className:t,children:(0,p.jsxs)("div",{className:o,children:[(0,p.jsx)("div",{className:s,children:(0,p.jsx)("svg",{viewBox:"0 0 24 24",width:"24",height:"24",fill:"currentColor",children:(0,p.jsx)("path",{d:"M12 1c-4.97 0-9 4.03-9 9v7c0 1.66 1.34 3 3 3h3v-8H5v-2c0-3.87 3.13-7 7-7s7 3.13 7 7v2h-4v8h3c1.66 0 3-1.34 3-3v-7c0-4.97-4.03-9-9-9z"})})}),(0,p.jsxs)("div",{className:a,children:[(0,p.jsx)("span",{className:l,children:"Prefer audio?"}),(0,p.jsxs)("span",{className:d,children:["Listen to Episode ",e]}),(0,p.jsxs)("span",{className:c,children:[r," listen"]})]}),(0,p.jsxs)(i.A,{to:n,className:m,children:[(0,p.jsx)("svg",{viewBox:"0 0 24 24",width:"20",height:"20",fill:"currentColor",children:(0,p.jsx)("path",{d:"M8 5v14l11-7z"})}),"Listen Now"]})]})})}},5831(n,e,r){r.r(e),r.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vla-models/index","title":"Vision-Language-Action Models","description":"Understanding and deploying VLA models for robot control","source":"@site/docs/vla-models/index.md","sourceDirName":"vla-models","slug":"/vla-models/","permalink":"/physical-ai-textbook/docs/vla-models/","draft":false,"unlisted":false,"editUrl":"https://github.com/Mansoor-Siddiqui/physical-ai-textbook/tree/master/physical-ai-textbook/docs/vla-models/index.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Vision-Language-Action Models","description":"Understanding and deploying VLA models for robot control","keywords":["vla","vision language action","rt-2","palm-e","foundation models","robotics"]},"sidebar":"textbookSidebar","previous":{"title":"NVIDIA Isaac Sim","permalink":"/physical-ai-textbook/docs/isaac-sim/"},"next":{"title":"Humanoid Robotics","permalink":"/physical-ai-textbook/docs/humanoid/"}}');var t=r(4848),o=r(8453),s=r(5168);const a={sidebar_position:5,title:"Vision-Language-Action Models",description:"Understanding and deploying VLA models for robot control",keywords:["vla","vision language action","rt-2","palm-e","foundation models","robotics"]},l="Vision-Language-Action Models",d={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"The VLA Paradigm",id:"the-vla-paradigm",level:3},{value:"Key Advantages",id:"key-advantages",level:3},{value:"Architecture Deep Dive",id:"architecture-deep-dive",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Action Tokenization",id:"action-tokenization",level:3},{value:"Major VLA Models",id:"major-vla-models",level:2},{value:"RT-2 (Robotics Transformer 2)",id:"rt-2-robotics-transformer-2",level:3},{value:"PaLM-E",id:"palm-e",level:3},{value:"OpenVLA",id:"openvla",level:3},{value:"Training VLA Models",id:"training-vla-models",level:2},{value:"Data Collection",id:"data-collection",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Inference Pipeline",id:"inference-pipeline",level:3},{value:"Safety Wrapper",id:"safety-wrapper",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Metrics",id:"metrics",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vision-language-action-models",children:"Vision-Language-Action Models"})}),"\n",(0,t.jsxs)("div",{className:"learning-objectives",children:[(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the architecture and principles of VLA models"}),"\n",(0,t.jsx)(e.li,{children:"Explain how vision, language, and action are unified in a single model"}),"\n",(0,t.jsx)(e.li,{children:"Compare different VLA architectures (RT-2, PaLM-E, OpenVLA)"}),"\n",(0,t.jsx)(e.li,{children:"Implement basic VLA inference pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Understand training strategies for VLA models"}),"\n",(0,t.jsx)(e.li,{children:"Deploy VLA models on robot hardware"}),"\n"]})]}),"\n",(0,t.jsx)(e.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Vision-Language-Action (VLA) models"})," are foundation models that unify visual perception, natural language understanding, and robot action generation in a single neural network. They represent a paradigm shift from traditional robotics pipelines to end-to-end learned systems."]}),"\n",(0,t.jsx)(e.h3,{id:"the-vla-paradigm",children:"The VLA Paradigm"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                     TRADITIONAL PIPELINE                         \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\r\n\u2502  \u2502 Vision \u2502\u2500\u2500\u25b6\u2502 Planning \u2502\u2500\u2500\u25b6\u2502 Motion \u2502\u2500\u2500\u25b6\u2502 Control  \u2502         \u2502\r\n\u2502  \u2502 System \u2502   \u2502  System  \u2502   \u2502Planner \u2502   \u2502  System  \u2502         \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\r\n\u2502      \u25b2                                                          \u2502\r\n\u2502      \u2502        Separate modules, hand-designed interfaces        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                        VLA MODEL                                 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                  \u2502\r\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502   \u2502              UNIFIED TRANSFORMER MODEL                    \u2502  \u2502\r\n\u2502   \u2502                                                           \u2502  \u2502\r\n\u2502   \u2502   Image \u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u2502  \u2502\r\n\u2502   \u2502               \u2502                                          \u2502  \u2502\r\n\u2502   \u2502   Language \u2500\u2500\u2500\u253c\u2500\u2500\u25b6 [Transformer] \u2500\u2500\u25b6 Robot Actions       \u2502  \u2502\r\n\u2502   \u2502               \u2502                                          \u2502  \u2502\r\n\u2502   \u2502   Robot State\u2500\u2518                                          \u2502  \u2502\r\n\u2502   \u2502                                                           \u2502  \u2502\r\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2502              End-to-end learning, emergent behaviors             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"key-advantages",children:"Key Advantages"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Advantage"}),(0,t.jsx)(e.th,{children:"Description"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Generalization"})}),(0,t.jsx)(e.td,{children:"Transfer knowledge across tasks and environments"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Language Grounding"})}),(0,t.jsx)(e.td,{children:"Natural language instructions directly control behavior"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Emergent Behaviors"})}),(0,t.jsx)(e.td,{children:"Complex skills emerge from data, not engineering"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Scalability"})}),(0,t.jsx)(e.td,{children:"Performance improves with more data and compute"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Flexibility"})}),(0,t.jsx)(e.td,{children:"Same model handles diverse tasks without reprogramming"})]})]})]}),"\n",(0,t.jsx)(e.h2,{id:"architecture-deep-dive",children:"Architecture Deep Dive"}),"\n",(0,t.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,t.jsx)(e.p,{children:"A VLA model typically consists of:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision Encoder"}),": Processes visual input into embeddings"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Model"}),": Understands instructions and reasons about tasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Decoder"}),": Generates robot control commands"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nfrom transformers import CLIPVisionModel, GPT2LMHeadModel\r\n\r\nclass SimpleVLA(nn.Module):\r\n    """Simplified VLA architecture for illustration"""\r\n    \r\n    def __init__(self, \r\n                 vision_model: str = "openai/clip-vit-base-patch32",\r\n                 language_model: str = "gpt2",\r\n                 action_dim: int = 7,\r\n                 hidden_dim: int = 768):\r\n        super().__init__()\r\n        \r\n        # Vision encoder (frozen CLIP)\r\n        self.vision_encoder = CLIPVisionModel.from_pretrained(vision_model)\r\n        for param in self.vision_encoder.parameters():\r\n            param.requires_grad = False\r\n        \r\n        # Vision projection\r\n        self.vision_projection = nn.Linear(\r\n            self.vision_encoder.config.hidden_size, \r\n            hidden_dim\r\n        )\r\n        \r\n        # Language model backbone\r\n        self.language_model = GPT2LMHeadModel.from_pretrained(language_model)\r\n        \r\n        # Action head\r\n        self.action_head = nn.Sequential(\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.GELU(),\r\n            nn.Linear(hidden_dim, action_dim)\r\n        )\r\n        \r\n    def forward(self, images, input_ids, attention_mask):\r\n        # Encode images\r\n        vision_outputs = self.vision_encoder(images)\r\n        image_embeds = vision_outputs.last_hidden_state[:, 0]  # CLS token\r\n        image_embeds = self.vision_projection(image_embeds)\r\n        \r\n        # Get language embeddings\r\n        token_embeds = self.language_model.transformer.wte(input_ids)\r\n        \r\n        # Prepend image embeddings\r\n        combined = torch.cat([\r\n            image_embeds.unsqueeze(1),\r\n            token_embeds\r\n        ], dim=1)\r\n        \r\n        # Forward through transformer\r\n        outputs = self.language_model.transformer(\r\n            inputs_embeds=combined,\r\n            attention_mask=torch.cat([\r\n                torch.ones(images.shape[0], 1, device=images.device),\r\n                attention_mask\r\n            ], dim=1)\r\n        )\r\n        \r\n        # Generate action from last hidden state\r\n        last_hidden = outputs.last_hidden_state[:, -1]\r\n        actions = self.action_head(last_hidden)\r\n        \r\n        return actions\n'})}),"\n",(0,t.jsx)(e.h3,{id:"action-tokenization",children:"Action Tokenization"}),"\n",(0,t.jsx)(e.p,{children:"VLA models often discretize continuous actions into tokens:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class ActionTokenizer:\r\n    """Discretize continuous actions into tokens"""\r\n    \r\n    def __init__(self, \r\n                 action_dim: int,\r\n                 num_bins: int = 256,\r\n                 action_min: float = -1.0,\r\n                 action_max: float = 1.0):\r\n        self.action_dim = action_dim\r\n        self.num_bins = num_bins\r\n        self.action_min = action_min\r\n        self.action_max = action_max\r\n        \r\n        # Bin edges\r\n        self.bin_edges = torch.linspace(\r\n            action_min, action_max, num_bins + 1\r\n        )\r\n    \r\n    def encode(self, actions: torch.Tensor) -> torch.Tensor:\r\n        """Convert continuous actions to tokens"""\r\n        # Clamp to valid range\r\n        actions = torch.clamp(actions, self.action_min, self.action_max)\r\n        \r\n        # Quantize to bins\r\n        tokens = torch.bucketize(actions, self.bin_edges[:-1]) - 1\r\n        tokens = torch.clamp(tokens, 0, self.num_bins - 1)\r\n        \r\n        return tokens\r\n    \r\n    def decode(self, tokens: torch.Tensor) -> torch.Tensor:\r\n        """Convert tokens back to continuous actions"""\r\n        # Get bin centers\r\n        bin_centers = (self.bin_edges[:-1] + self.bin_edges[1:]) / 2\r\n        \r\n        # Map tokens to actions\r\n        actions = bin_centers[tokens]\r\n        \r\n        return actions\n'})}),"\n",(0,t.jsx)(e.h2,{id:"major-vla-models",children:"Major VLA Models"}),"\n",(0,t.jsx)(e.h3,{id:"rt-2-robotics-transformer-2",children:"RT-2 (Robotics Transformer 2)"}),"\n",(0,t.jsx)(e.p,{children:"Google DeepMind's RT-2 demonstrated that vision-language models can directly output robot actions."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'RT-2 Architecture:\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502           PaLI-X or PaLM-E Backbone             \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                 \u2502\r\n\u2502   Image \u2500\u2500\u25b6 ViT Encoder \u2500\u2500\u2510                    \u2502\r\n\u2502                           \u251c\u2500\u2500\u25b6 Transformer \u2500\u2500\u25b6 \u2502\r\n\u2502   "Pick up the apple" \u2500\u2500\u2500\u2500\u2518                    \u2502\r\n\u2502                                                 \u2502\r\n\u2502   Output: "1 128 91 241 5 101 127"             \u2502\r\n\u2502           (action tokens)                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,t.jsx)(e.p,{children:"Key features:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Co-trained on web-scale vision-language data and robotics data"}),"\n",(0,t.jsx)(e.li,{children:'Actions represented as text tokens (e.g., "1 128 91")'}),"\n",(0,t.jsx)(e.li,{children:"Chain-of-thought reasoning emerges naturally"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"palm-e",children:"PaLM-E"}),"\n",(0,t.jsx)(e.p,{children:"Google's PaLM-E integrates embodied knowledge into a large language model."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Conceptual PaLM-E forward pass\r\ndef palm_e_forward(image, language_instruction, robot_state):\r\n    """\r\n    PaLM-E processes multi-modal inputs and generates plans\r\n    """\r\n    # Encode image into "visual sentences"\r\n    visual_tokens = vit_encoder(image)\r\n    \r\n    # Encode robot state as special tokens\r\n    state_tokens = state_encoder(robot_state)\r\n    \r\n    # Interleave with language\r\n    prompt = f"""\r\n    <image>{visual_tokens}</image>\r\n    Robot state: <state>{state_tokens}</state>\r\n    Task: {language_instruction}\r\n    Plan:\r\n    """\r\n    \r\n    # Generate plan as natural language\r\n    plan = palm_decoder(prompt)\r\n    # e.g., "1. Move arm above red block\r\n    #        2. Lower gripper\r\n    #        3. Close gripper\r\n    #        4. Lift arm"\r\n    \r\n    return plan\n'})}),"\n",(0,t.jsx)(e.h3,{id:"openvla",children:"OpenVLA"}),"\n",(0,t.jsx)(e.p,{children:"An open-source VLA model for research:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Using OpenVLA for inference\r\nfrom openvla import OpenVLA\r\nfrom PIL import Image\r\n\r\n# Load model\r\nmodel = OpenVLA.from_pretrained("openvla/openvla-7b")\r\n\r\n# Prepare inputs\r\nimage = Image.open("scene.jpg")\r\ninstruction = "Pick up the red cup and place it on the blue plate"\r\n\r\n# Get action\r\naction = model.predict_action(\r\n    image=image,\r\n    instruction=instruction,\r\n    unnorm_key="bridge_orig"  # Dataset-specific normalization\r\n)\r\n\r\n# Action format: [x, y, z, roll, pitch, yaw, gripper]\r\nprint(f"Predicted action: {action}")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"training-vla-models",children:"Training VLA Models"}),"\n",(0,t.jsx)(e.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,t.jsx)(e.p,{children:"VLA training requires diverse robot demonstration data:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class RobotDemonstrationDataset:\r\n    \"\"\"Dataset of robot demonstrations\"\"\"\r\n    \r\n    def __init__(self, data_path: str):\r\n        self.episodes = self.load_episodes(data_path)\r\n    \r\n    def load_episodes(self, path):\r\n        \"\"\"Load demonstration episodes\"\"\"\r\n        episodes = []\r\n        for ep_file in Path(path).glob(\"*.hdf5\"):\r\n            with h5py.File(ep_file, 'r') as f:\r\n                episode = {\r\n                    'images': f['observations/images/front'][:],\r\n                    'actions': f['actions'][:],\r\n                    'language': f.attrs['language_instruction']\r\n                }\r\n                episodes.append(episode)\r\n        return episodes\r\n    \r\n    def __getitem__(self, idx):\r\n        episode = self.episodes[idx]\r\n        \r\n        # Sample a random timestep\r\n        t = np.random.randint(len(episode['actions']))\r\n        \r\n        return {\r\n            'image': self.preprocess_image(episode['images'][t]),\r\n            'instruction': episode['language'],\r\n            'action': episode['actions'][t]\r\n        }\n"})}),"\n",(0,t.jsx)(e.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from torch.utils.data import DataLoader\r\nfrom transformers import AdamW, get_cosine_schedule_with_warmup\r\n\r\ndef train_vla(model, dataset, config):\r\n    \"\"\"Train VLA model on robot demonstrations\"\"\"\r\n    \r\n    dataloader = DataLoader(\r\n        dataset, \r\n        batch_size=config.batch_size,\r\n        shuffle=True,\r\n        num_workers=4\r\n    )\r\n    \r\n    optimizer = AdamW(\r\n        model.parameters(),\r\n        lr=config.learning_rate,\r\n        weight_decay=config.weight_decay\r\n    )\r\n    \r\n    scheduler = get_cosine_schedule_with_warmup(\r\n        optimizer,\r\n        num_warmup_steps=config.warmup_steps,\r\n        num_training_steps=config.total_steps\r\n    )\r\n    \r\n    model.train()\r\n    for step, batch in enumerate(dataloader):\r\n        # Forward pass\r\n        predicted_actions = model(\r\n            images=batch['image'].to(device),\r\n            input_ids=batch['input_ids'].to(device),\r\n            attention_mask=batch['attention_mask'].to(device)\r\n        )\r\n        \r\n        # Action prediction loss (MSE for continuous, CE for discrete)\r\n        if config.discrete_actions:\r\n            loss = F.cross_entropy(\r\n                predicted_actions.view(-1, config.num_bins),\r\n                batch['action_tokens'].view(-1)\r\n            )\r\n        else:\r\n            loss = F.mse_loss(predicted_actions, batch['action'])\r\n        \r\n        # Backward pass\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\r\n        optimizer.step()\r\n        scheduler.step()\r\n        \r\n        if step % config.log_interval == 0:\r\n            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n"})}),"\n",(0,t.jsx)(e.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import albumentations as A\r\nfrom albumentations.pytorch import ToTensorV2\r\n\r\ndef get_augmentation_pipeline():\r\n    """Augmentations for VLA training"""\r\n    return A.Compose([\r\n        # Spatial augmentations\r\n        A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\r\n        A.HorizontalFlip(p=0.5),\r\n        \r\n        # Color augmentations\r\n        A.ColorJitter(\r\n            brightness=0.2,\r\n            contrast=0.2,\r\n            saturation=0.2,\r\n            hue=0.1,\r\n            p=0.8\r\n        ),\r\n        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\r\n        \r\n        # Normalize and convert\r\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n        ToTensorV2()\r\n    ])\n'})}),"\n",(0,t.jsx)(e.h2,{id:"deployment",children:"Deployment"}),"\n",(0,t.jsx)(e.h3,{id:"inference-pipeline",children:"Inference Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import time\r\nfrom collections import deque\r\n\r\nclass VLAController:\r\n    """Deploy VLA model for real-time robot control"""\r\n    \r\n    def __init__(self, model_path: str, device: str = "cuda"):\r\n        self.device = device\r\n        self.model = self.load_model(model_path)\r\n        self.model.eval()\r\n        \r\n        # Action history for temporal smoothing\r\n        self.action_history = deque(maxlen=5)\r\n        \r\n        # Timing\r\n        self.control_frequency = 10  # Hz\r\n        \r\n    def load_model(self, path):\r\n        """Load and optimize model for inference"""\r\n        model = torch.load(path)\r\n        model = model.to(self.device)\r\n        \r\n        # Optional: TensorRT optimization\r\n        # model = torch.compile(model, mode="reduce-overhead")\r\n        \r\n        return model\r\n    \r\n    @torch.no_grad()\r\n    def get_action(self, image, instruction):\r\n        """Get action from observation"""\r\n        start_time = time.time()\r\n        \r\n        # Preprocess\r\n        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\r\n        tokens = self.tokenize(instruction).to(self.device)\r\n        \r\n        # Inference\r\n        action = self.model(\r\n            images=image_tensor,\r\n            input_ids=tokens[\'input_ids\'],\r\n            attention_mask=tokens[\'attention_mask\']\r\n        )\r\n        \r\n        action = action.cpu().numpy()[0]\r\n        \r\n        # Temporal smoothing\r\n        self.action_history.append(action)\r\n        smoothed_action = np.mean(self.action_history, axis=0)\r\n        \r\n        # Log timing\r\n        inference_time = time.time() - start_time\r\n        if inference_time > 1.0 / self.control_frequency:\r\n            print(f"Warning: Inference too slow ({inference_time:.3f}s)")\r\n        \r\n        return smoothed_action\r\n    \r\n    def run(self, robot, camera, instruction):\r\n        """Main control loop"""\r\n        rate = 1.0 / self.control_frequency\r\n        \r\n        while True:\r\n            loop_start = time.time()\r\n            \r\n            # Get observation\r\n            image = camera.get_image()\r\n            \r\n            # Get action\r\n            action = self.get_action(image, instruction)\r\n            \r\n            # Execute action\r\n            robot.execute_action(action)\r\n            \r\n            # Maintain control frequency\r\n            elapsed = time.time() - loop_start\r\n            if elapsed < rate:\r\n                time.sleep(rate - elapsed)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"safety-wrapper",children:"Safety Wrapper"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class SafeVLAController:\r\n    """VLA controller with safety checks"""\r\n    \r\n    def __init__(self, vla_controller, safety_config):\r\n        self.controller = vla_controller\r\n        self.config = safety_config\r\n        \r\n    def get_safe_action(self, image, instruction):\r\n        """Get action with safety filtering"""\r\n        # Get raw action\r\n        action = self.controller.get_action(image, instruction)\r\n        \r\n        # Check velocity limits\r\n        action[:3] = np.clip(\r\n            action[:3], \r\n            -self.config.max_velocity, \r\n            self.config.max_velocity\r\n        )\r\n        \r\n        # Check workspace bounds\r\n        # (requires current robot state)\r\n        \r\n        # Check for collision (simplified)\r\n        if self.predict_collision(action):\r\n            print("Collision predicted, stopping!")\r\n            return np.zeros_like(action)\r\n        \r\n        return action\r\n    \r\n    def predict_collision(self, action):\r\n        """Simple collision check (placeholder)"""\r\n        # In practice, use collision detection library\r\n        return False\n'})}),"\n",(0,t.jsx)(e.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,t.jsx)(e.h3,{id:"metrics",children:"Metrics"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"def evaluate_vla(model, eval_episodes, env):\r\n    \"\"\"Evaluate VLA model on test episodes\"\"\"\r\n    \r\n    metrics = {\r\n        'success_rate': [],\r\n        'completion_time': [],\r\n        'action_smoothness': []\r\n    }\r\n    \r\n    for episode in eval_episodes:\r\n        obs = env.reset(episode['initial_state'])\r\n        instruction = episode['instruction']\r\n        \r\n        done = False\r\n        actions = []\r\n        start_time = time.time()\r\n        \r\n        while not done:\r\n            action = model.get_action(obs['image'], instruction)\r\n            actions.append(action)\r\n            \r\n            obs, reward, done, info = env.step(action)\r\n        \r\n        # Record metrics\r\n        metrics['success_rate'].append(info['success'])\r\n        metrics['completion_time'].append(time.time() - start_time)\r\n        \r\n        # Action smoothness (jerk)\r\n        actions = np.array(actions)\r\n        jerk = np.diff(actions, n=2, axis=0)\r\n        metrics['action_smoothness'].append(np.mean(np.abs(jerk)))\r\n    \r\n    # Aggregate\r\n    return {\r\n        'success_rate': np.mean(metrics['success_rate']),\r\n        'avg_completion_time': np.mean(metrics['completion_time']),\r\n        'avg_smoothness': np.mean(metrics['action_smoothness'])\r\n    }\n"})}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"VLA models"})," unify vision, language, and action in end-to-end systems"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Architectures"})," typically combine vision encoders, language models, and action decoders"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Training"})," requires large-scale robot demonstration data with language annotations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Deployment"})," requires optimization for real-time control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"})," is critical when deploying learned policies on real robots"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"In the next chapter, we'll apply VLA concepts to humanoid robotics, exploring the unique challenges of bipedal locomotion and human-like manipulation."}),"\n",(0,t.jsx)(e.hr,{}),"\n","\n",(0,t.jsx)(s.A,{episodeUrl:"/podcast/episodes/ep05-vla-models",episodeNumber:5,duration:"19 min"})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453(n,e,r){r.d(e,{R:()=>s,x:()=>a});var i=r(6540);const t={},o=i.createContext(t);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);