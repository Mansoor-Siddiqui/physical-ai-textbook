<!doctype html>
<html lang="en-US" dir="ltr" class="docs-wrapper plugin-docs plugin-id-podcast docs-version-current docs-doc-page docs-doc-id-episodes/ep05-vla-models" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Episode 5: Vision-Language-Action Models | Physical AI Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://mansoor-siddiqui.github.io/physical-ai-textbook/img/physical-ai-social-card.png"><meta data-rh="true" name="twitter:image" content="https://mansoor-siddiqui.github.io/physical-ai-textbook/img/physical-ai-social-card.png"><meta data-rh="true" property="og:url" content="https://mansoor-siddiqui.github.io/physical-ai-textbook/podcast/episodes/ep05-vla-models"><meta data-rh="true" property="og:locale" content="en_US"><meta data-rh="true" property="og:locale:alternate" content="ur_PK"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="og:image:width" content="1200"><meta data-rh="true" name="og:image:height" content="630"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-podcast-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-podcast-current"><meta data-rh="true" property="og:title" content="Episode 5: Vision-Language-Action Models | Physical AI Textbook"><meta data-rh="true" name="description" content="Understanding VLA models that unify perception, language, and robot action"><meta data-rh="true" property="og:description" content="Understanding VLA models that unify perception, language, and robot action"><link data-rh="true" rel="icon" href="/physical-ai-textbook/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mansoor-siddiqui.github.io/physical-ai-textbook/podcast/episodes/ep05-vla-models"><link data-rh="true" rel="alternate" href="https://mansoor-siddiqui.github.io/physical-ai-textbook/podcast/episodes/ep05-vla-models" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://mansoor-siddiqui.github.io/physical-ai-textbook/ur/podcast/episodes/ep05-vla-models" hreflang="ur-PK"><link data-rh="true" rel="alternate" href="https://mansoor-siddiqui.github.io/physical-ai-textbook/podcast/episodes/ep05-vla-models" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Episode 5: Vision-Language-Action Models","item":"https://mansoor-siddiqui.github.io/physical-ai-textbook/podcast/episodes/ep05-vla-models"}]}</script><meta name="description" content="Comprehensive educational platform for Physical AI and Humanoid Robotics. Learn ROS2, Gazebo simulation, NVIDIA Isaac Sim, and Vision-Language-Action models with hands-on labs and podcasts.">
<meta name="keywords" content="Physical AI, Humanoid Robotics, ROS2, Gazebo, NVIDIA Isaac Sim, VLA Models, Robot Simulation, Embodied AI, Machine Learning, Robotics Tutorial">
<meta name="author" content="Physical AI Textbook Team">
<meta name="robots" content="index, follow">
<meta property="og:type" content="website">
<meta property="og:site_name" content="Physical AI Textbook">
<meta property="og:locale" content="en_US">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:creator" content="@physicalai">
<meta name="theme-color" content="#1a1a2e">
<link rel="apple-touch-icon" sizes="180x180" href="/img/apple-touch-icon.png">
<script type="application/ld+json">{"@context":"https://schema.org","@type":"Course","name":"Physical AI Textbook","description":"Comprehensive educational platform for Physical AI and Humanoid Robotics covering ROS2, Gazebo, NVIDIA Isaac Sim, and Vision-Language-Action models.","provider":{"@type":"Organization","name":"Physical AI Textbook"},"educationalLevel":"Intermediate","teaches":["ROS2 Fundamentals","Robot Simulation with Gazebo","NVIDIA Isaac Sim","Vision-Language-Action Models","Humanoid Robotics"],"hasCourseInstance":{"@type":"CourseInstance","courseMode":"online","courseWorkload":"PT20H"}}</script><link rel="stylesheet" href="/physical-ai-textbook/assets/css/styles.877fc3ba.css">
<script src="/physical-ai-textbook/assets/js/runtime~main.09a56473.js" defer="defer"></script>
<script src="/physical-ai-textbook/assets/js/main.823bed14.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><link rel="preload" as="image" href="/physical-ai-textbook/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#1a1a2e;color:#e94560" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">ðŸ¤– New: <a target="_blank" rel="noopener noreferrer" href="/podcast">Listen to our Physical AI Podcast</a> - Audio versions of all chapters!</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-textbook/"><div class="navbar__logo"><img src="/physical-ai-textbook/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/physical-ai-textbook/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI Textbook</b></a><a class="navbar__item navbar__link" href="/physical-ai-textbook/docs/intro">Textbook</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/physical-ai-textbook/podcast">Podcast</a><a class="navbar__item navbar__link" href="/physical-ai-textbook/docs/labs/lab01-ros2-hello">Labs</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/physical-ai-textbook/podcast/episodes/ep05-vla-models" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en-US">English</a></li><li><a href="/physical-ai-textbook/ur/podcast/episodes/ep05-vla-models" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ur-PK">Ø§Ø±Ø¯Ùˆ</a></li></ul></div><a href="https://github.com/Mansoor-Siddiqui/physical-ai-textbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="container_XvAR"><div class="loadingDot_Np_j"></div></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-textbook/podcast"><span title="All Episodes" class="linkLabel_WmDU">All Episodes</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/physical-ai-textbook/podcast/episodes/ep01-physical-ai"><span title="Episodes" class="categoryLinkLabel_W154">Episodes</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep01-physical-ai"><span title="Episode 1: Introduction to Physical AI" class="linkLabel_WmDU">Episode 1: Introduction to Physical AI</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep02-ros2"><span title="Episode 2: ROS2 Fundamentals" class="linkLabel_WmDU">Episode 2: ROS2 Fundamentals</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep03-simulation"><span title="Episode 3: Robot Simulation" class="linkLabel_WmDU">Episode 3: Robot Simulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep04-isaac-sim"><span title="Episode 4: NVIDIA Isaac Sim" class="linkLabel_WmDU">Episode 4: NVIDIA Isaac Sim</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep05-vla-models"><span title="Episode 5: Vision-Language-Action Models" class="linkLabel_WmDU">Episode 5: Vision-Language-Action Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-textbook/podcast/episodes/ep06-humanoid"><span title="Episode 6: Humanoid Robotics" class="linkLabel_WmDU">Episode 6: Humanoid Robotics</span></a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/physical-ai-textbook/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Episodes</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Episode 5: Vision-Language-Action Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Episode 5: Vision-Language-Action Models</h1></header>
<!-- -->
<div class="podcastPlayer_Lsz6"><div class="header_C16S"><div class="episodeBadge_noaF">5</div><div class="titleSection_l4ro"><h3 class="title_r2SD">Vision-Language-Action Models</h3><span class="duration_Qddz">19 min</span></div></div><div class="controls_h1ku"><button class="restartButton_NB1M" aria-label="Restart" disabled="" title="Restart"><svg viewBox="0 0 24 24" width="20" height="20" fill="currentColor"><path d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"></path></svg></button><button class="playButton_lnHZ" aria-label="Play" disabled=""><svg viewBox="0 0 24 24" width="24" height="24" fill="currentColor"><path d="M8 5v14l11-7z"></path></svg></button><div class="progressSection_W78N"><span class="time_ejeR">0:00</span><input type="range" class="progressBar_GOIJ" min="0" max="100" disabled="" value="0"><span class="time_ejeR">0:00</span></div></div><div class="secondaryControls_H77O"><div class="speedControls_a9vH"><span class="label_lm2X">Speed:</span><button class="speedButton_W33j" disabled="">0.5<!-- -->x</button><button class="speedButton_W33j" disabled="">0.75<!-- -->x</button><button class="speedButton_W33j active_YM1_" disabled="">1<!-- -->x</button><button class="speedButton_W33j" disabled="">1.25<!-- -->x</button><button class="speedButton_W33j" disabled="">1.5<!-- -->x</button><button class="speedButton_W33j" disabled="">2<!-- -->x</button></div><div class="volumeControl_dGTa"><svg viewBox="0 0 24 24" width="16" height="16" fill="currentColor"><path d="M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02z"></path></svg><input type="range" class="volumeSlider_SO7d" min="0" max="1" step="0.1" disabled="" value="1"></div></div><div class="placeholder_GszZ"><p>Audio not available. Your browser doesn&#x27;t support Text-to-Speech.</p></div><details class="showNotes_xvMe"><summary>Show Notes</summary><ul><li>The VLA paradigm vs traditional modular robotics</li><li>Architecture: vision encoder, language model, action decoder</li><li>RT-2 and PaLM-E breakthroughs</li><li>Action tokenization - actions as language</li><li>Training at scale with large datasets</li><li>Deployment challenges and the future</li></ul></details></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="episode-summary">Episode Summary<a href="#episode-summary" class="hash-link" aria-label="Direct link to Episode Summary" title="Direct link to Episode Summary" translate="no">â€‹</a></h2>
<p>Vision-Language-Action models represent a paradigm shift in robotics. Instead of separate perception, planning, and control modules, VLAs unify everything into a single neural network. This episode explores how these models work, why they matter, and how they&#x27;re changing robot capabilities.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-topics-covered">Key Topics Covered<a href="#key-topics-covered" class="hash-link" aria-label="Direct link to Key Topics Covered" title="Direct link to Key Topics Covered" translate="no">â€‹</a></h2>
<ul>
<li class=""><strong>The VLA Paradigm</strong>: From modular pipelines to end-to-end learning</li>
<li class=""><strong>Architecture</strong>: Vision encoders, language models, and action decoders</li>
<li class=""><strong>RT-2 and PaLM-E</strong>: Major research breakthroughs</li>
<li class=""><strong>Action Tokenization</strong>: Representing robot motions as language</li>
<li class=""><strong>Training at Scale</strong>: Large datasets and compute requirements</li>
<li class=""><strong>Deployment Challenges</strong>: Real-time inference on robots</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="show-notes">Show Notes<a href="#show-notes" class="hash-link" aria-label="Direct link to Show Notes" title="Direct link to Show Notes" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-vla-models">Key VLA Models<a href="#key-vla-models" class="hash-link" aria-label="Direct link to Key VLA Models" title="Direct link to Key VLA Models" translate="no">â€‹</a></h3>
<table><thead><tr><th>Model</th><th>Creator</th><th>Key Innovation</th></tr></thead><tbody><tr><td><strong>RT-2</strong></td><td>Google DeepMind</td><td>Actions as language tokens</td></tr><tr><td><strong>PaLM-E</strong></td><td>Google</td><td>Multimodal embodied LLM</td></tr><tr><td><strong>OpenVLA</strong></td><td>Open-source</td><td>Accessible VLA for research</td></tr><tr><td><strong>Octo</strong></td><td>UC Berkeley</td><td>Generalist robot policy</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-vla-advantage">The VLA Advantage<a href="#the-vla-advantage" class="hash-link" aria-label="Direct link to The VLA Advantage" title="Direct link to The VLA Advantage" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Generalization</strong>: One model handles many tasks</li>
<li class=""><strong>Language Interface</strong>: Natural instructions control robots</li>
<li class=""><strong>Emergent Abilities</strong>: Complex behaviors from scale</li>
<li class=""><strong>Transfer Learning</strong>: Web knowledge helps robotics</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="resources">Resources<a href="#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources" translate="no">â€‹</a></h3>
<ul>
<li class=""><a class="" href="/physical-ai-textbook/docs/vla-models">Chapter: Vision-Language-Action Models</a></li>
<li class=""><a href="https://arxiv.org/abs/2307.15818" target="_blank" rel="noopener noreferrer" class="">RT-2 Paper</a></li>
<li class=""><a href="https://robotics-transformer-x.github.io/" target="_blank" rel="noopener noreferrer" class="">Open X-Embodiment</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="transcript">Transcript<a href="#transcript" class="hash-link" aria-label="Direct link to Transcript" title="Direct link to Transcript" translate="no">â€‹</a></h2>
<p><strong>[INTRO MUSIC]</strong></p>
<p>Welcome to the Physical AI Podcast! Today we&#x27;re exploring what might be the most exciting development in robotics AI: Vision-Language-Action models, or VLAs.</p>
<p><strong>[SECTION 1: The Traditional Approach]</strong></p>
<p>Let&#x27;s start with how robotics has worked traditionally. You have separate modules: a perception system that sees the world, a planning system that decides what to do, and a control system that executes motions. Each module is engineered independently with carefully designed interfaces between them.</p>
<p>This modular approach has advantages. Each component can be tested and improved independently. But it also has limitations. Information is lost at each boundary. The perception system doesn&#x27;t know what the planner needs. The planner doesn&#x27;t know the controller&#x27;s constraints. And making everything work together requires enormous engineering effort.</p>
<p><strong>[SECTION 2: The VLA Revolution]</strong></p>
<p>VLA models throw out the modular playbook. Instead, a single neural network goes directly from camera pixels and language instructions to robot actions. See, understand, act - all in one forward pass.</p>
<p>This is possible because of two converging trends: large vision-language models like GPT-4V that can understand images and text together, and massive robot demonstration datasets that show how tasks should be performed.</p>
<p>The key insight is treating robot actions like language. Just as a language model predicts the next word, a VLA predicts the next action. The same transformer architecture handles both.</p>
<p><strong>[SECTION 3: Inside the Architecture]</strong></p>
<p>Let&#x27;s peek inside a VLA. There are typically three components.</p>
<p>First, a vision encoder processes camera images. This might be a Vision Transformer that converts images into embedding vectors. Often, this encoder is pre-trained on millions of images from the internet.</p>
<p>Second, a language model backbone processes text instructions and reasons about the task. Large models like PaLM bring vast world knowledge - they understand concepts, relationships, and common sense that aren&#x27;t in robot datasets.</p>
<p>Third, an action head converts the model&#x27;s understanding into robot commands. This might output joint positions, end-effector poses, or velocity commands.</p>
<p>The magic happens when these components are trained together on robot demonstrations. The model learns to connect language concepts to visual features to appropriate actions.</p>
<p><strong>[SECTION 4: RT-2 - A Case Study]</strong></p>
<p>Google&#x27;s RT-2 was a watershed moment. They took a large vision-language model - PaLI-X with 55 billion parameters - and fine-tuned it on robot data.</p>
<p>The clever trick was representing actions as text. Instead of a separate action output, RT-2 generates strings like &quot;1 128 91 241 5 101 127&quot; where each number is a discretized component of the robot action. The language model&#x27;s text generation capability directly produces robot motions.</p>
<p>The results were remarkable. RT-2 could follow complex instructions, reason about objects it had never seen during robot training, and even perform simple math to choose which object to pick. Knowledge from internet pretraining transferred to robot capabilities.</p>
<p><strong>[SECTION 5: Training VLAs]</strong></p>
<p>Training VLAs requires significant resources. You need large robot demonstration datasets - thousands to millions of task examples. The Open X-Embodiment dataset combined data from many labs to create over a million demonstrations.</p>
<p>Compute requirements are substantial. Training a VLA from scratch requires clusters of GPUs or TPUs for weeks. This is why most research builds on pre-trained vision-language backbones.</p>
<p>Data quality matters enormously. Robot demonstrations need to be diverse, high-quality, and properly labeled with task descriptions. Collecting good data is often harder than designing the model.</p>
<p><strong>[SECTION 6: Deployment Reality]</strong></p>
<p>Deploying VLAs on real robots brings challenges. Inference must be fast - most robots need actions at 10-30 Hz. Large models are slow, especially on edge devices with limited compute.</p>
<p>Several strategies help. Model distillation compresses large VLAs into smaller, faster versions. Quantization reduces precision to speed up computation. Some teams use cloud inference, though latency can be an issue.</p>
<p>Safety is paramount. VLAs are black boxes - we don&#x27;t fully understand why they make particular decisions. Adding safety wrappers that check for dangerous actions is essential before real-world deployment.</p>
<p><strong>[SECTION 7: The Future]</strong></p>
<p>We&#x27;re early in the VLA era. Current models struggle with precise manipulation, long-horizon tasks, and truly novel situations. But progress is rapid.</p>
<p>I expect to see VLAs become standard in the next few years. As datasets grow and models scale, capabilities will expand. The vision of robots that understand natural language and perform diverse tasks is becoming reality.</p>
<p><strong>[OUTRO]</strong></p>
<p>VLAs represent a fundamental shift in how we build robot intelligence. Instead of engineering modules, we&#x27;re learning integrated systems end-to-end. It&#x27;s exciting and a little terrifying.</p>
<p>Next episode, we&#x27;re wrapping up with humanoid robotics - the ultimate physical AI challenge. Bipedal walking, human-like manipulation, and the race to build general-purpose humanoids.</p>
<p>See you then!</p>
<p><strong>[OUTRO MUSIC]</strong></p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="related-content">Related Content<a href="#related-content" class="hash-link" aria-label="Direct link to Related Content" title="Direct link to Related Content" translate="no">â€‹</a></h2>
<ul>
<li class=""><strong>Previous Episode</strong>: <a class="" href="/physical-ai-textbook/podcast/episodes/ep04-isaac-sim">Episode 4: NVIDIA Isaac Sim</a></li>
<li class=""><strong>Next Episode</strong>: <a class="" href="/physical-ai-textbook/podcast/episodes/ep06-humanoid">Episode 6: Humanoid Robotics</a></li>
<li class=""><strong>Read the Chapter</strong>: <a class="" href="/physical-ai-textbook/docs/vla-models">Vision-Language-Action Models</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Mansoor-Siddiqui/physical-ai-textbook/tree/master/physical-ai-textbook/podcast/episodes/ep05-vla-models.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/physical-ai-textbook/podcast/episodes/ep04-isaac-sim"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Episode 4: NVIDIA Isaac Sim</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/physical-ai-textbook/podcast/episodes/ep06-humanoid"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Episode 6: Humanoid Robotics</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#episode-summary" class="table-of-contents__link toc-highlight">Episode Summary</a></li><li><a href="#key-topics-covered" class="table-of-contents__link toc-highlight">Key Topics Covered</a></li><li><a href="#show-notes" class="table-of-contents__link toc-highlight">Show Notes</a><ul><li><a href="#key-vla-models" class="table-of-contents__link toc-highlight">Key VLA Models</a></li><li><a href="#the-vla-advantage" class="table-of-contents__link toc-highlight">The VLA Advantage</a></li><li><a href="#resources" class="table-of-contents__link toc-highlight">Resources</a></li></ul></li><li><a href="#transcript" class="table-of-contents__link toc-highlight">Transcript</a></li><li><a href="#related-content" class="table-of-contents__link toc-highlight">Related Content</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-textbook/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-textbook/docs/ros2">ROS2 Fundamentals</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-textbook/docs/labs/lab01-ros2-hello">Labs</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Podcast</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-textbook/podcast">All Episodes</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-textbook/podcast/episodes/ep01-physical-ai">Latest Episode</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Mansoor-Siddiqui/physical-ai-textbook/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discourse.ros.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS Discourse<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Developer Forums<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Mansoor-Siddiqui/physical-ai-textbook" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://developer.nvidia.com/isaac-sim" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2026 Physical AI Textbook. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>